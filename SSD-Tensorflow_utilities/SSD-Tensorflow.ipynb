{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"SSD-Tensorflow.ipynb","provenance":[],"mount_file_id":"1jDPOW8AihQHfjwemBGzl-MgjAvUIP5bx","authorship_tag":"ABX9TyM9sD8/TbTXO6ZQJ7f2CnhR"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"tPdJ9ahvYRWO"},"source":["#SSD-Tensorflow\n","This notebook provides the training instructions for SSD-Tensorflow. Using two different type of pre-trained weights. Each section contains the instructions to setup corrrectly the net, the dataset was prepared in tfrecord format.\n","\n","Make sure that Colab runtime is setted up with GPU.\n","\n","Remember that Google Colaboratory provides a maximum running time of twelve hours for a single training section, then the training has to bo restarted.\n","\n","Further informations about SSD-Tensorflow are available at: https://github.com/balancap/SSD-Tensorflow"]},{"cell_type":"markdown","metadata":{"id":"eEtzK-jIZdkE"},"source":["#1. Repository Setup\n","As for the others notebooks of this repository, we download all materials in the \"/home/\" directory.\n","\n","Firstly we clone SSD-Tensorflow repository."]},{"cell_type":"code","metadata":{"id":"o5DfGArHYo00"},"source":["%cd /home/\n","!git clone https://github.com/balancap/SSD-Tensorflow.git"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZLSeDOvLZ8wE"},"source":["Almost all SSD-Tensorflow files use \"contrib\", a command that is no longer available in Tensorflow 2.x, so we need to force Colab to use the 1.x version of it."]},{"cell_type":"code","metadata":{"id":"fIojNRJaj4mZ"},"source":["%tensorflow_version 1.x\n","import tensorflow\n","print(tensorflow.__version__)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AtDFtto8a4bZ"},"source":["After that we install all requirements, in order to use all models needed in SSD-Tensorflow repository."]},{"cell_type":"code","metadata":{"id":"ecpHEnka8Kix"},"source":["%cd /content\n","!git clone --quiet https://github.com/tensorflow/models.git\n","\n","!apt-get install -qq protobuf-compiler python-pil python-lxml python-tk\n","\n","!pip install -q Cython contextlib2 pillow lxml matplotlib\n","\n","!pip install -q pycocotools\n","\n","!pip install git+https://github.com/google-research/tf-slim\n","\n","%cd /content/models/research\n","!protoc object_detection/protos/*.proto --python_out=.\n","\n","import os\n","os.environ['PYTHONPATH'] += ':/content/models/research/:/content/models/research/slim/'\n","\n","import os\n","os.environ['PYTHONPATH'] += \":/content/models\"\n","\n","import sys\n","sys.path.append(\"/content/models\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WZQTcxnTbLJo"},"source":["#2. Pre-Trained Weights\n","In this section we download and unzip pre-treined files. Not all of this models are request, you have to chose one at least.\n","SSD-Tensoflow repository is provided with \"ssd_300\" pre-treined model, so for that we have only to unzip it. "]},{"cell_type":"markdown","metadata":{"id":"BguybL1BcVkf"},"source":["ssd_300"]},{"cell_type":"code","metadata":{"id":"XkxiFXceZE4o"},"source":["%cd /home/SSD-Tensorflow/checkpoints/\n","!unzip ssd_300_vgg.ckpt.zip"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rbhXvKQrcXwB"},"source":["ssd_512"]},{"cell_type":"code","metadata":{"id":"9xgO2kQYHMBn"},"source":["%cd /home/SSD-Tensorflow/checkpoints/\n","!gdown --id 0B0qPCUZ-3YwWT1RCLVZNN3RTVEU\n","!unzip VGG_VOC0712_SSD_512x512_ft_iter_120000.ckpt.zip"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"H719ATPicdE2"},"source":["#3. dataset\n","SSD-Tensorflow is setup to run PascalVOC format dataset. We give to the net a tfrecord format converted from a KITTI format, in order to run it we need to upgrade SSD tensorflow. In the first part of this section we add new files that are able to recognize KITTI dataset."]},{"cell_type":"code","metadata":{"id":"AsKU1NlQ41qr"},"source":["%cd /home/SSD-Tensorflow/datasets/\n","%rm dataset_factory.py\n","%rm dataset_utils.py"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"coSu6eO4JMiO"},"source":["!gdown --id 15nddpLEe_DwqFeNhrVMIsKF8lcYADl6g\n","!gdown --id 15nIPB3AMr591nfDCe6hIfAUXlxWhMs6Z\n","!gdown --id 1mYA9BQoYOJdfx7JT4h1DRfd5NhYPk3mC\n","!gdown --id 1Lo9TWGrgkkFWJe7hx1NobgsZowkhNXOZ"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jTC6Es15eNP_"},"source":["In the second part, we create a new folder inside the repository where we will download and unzip the dataset. "]},{"cell_type":"code","metadata":{"id":"vPeQnJt1KF05"},"source":["%cd /home/SSD-Tensorflow/\n","!mkdir /home/SSD-Tensorflow/data\n","%cd ./data/\n","!gdown --id 1NrDwpC0XB8-V7by-sB2DEJbnBoBgTpk6\n","!unzip kitti_cones.zip"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"j695PrwJfB71"},"source":["#4. Weights Log\n","The trained weights will be saved in a folder created inside the home of your Google Drive disk. For that purpose, the notebook mount your Google Drive disk. In order to do it, you have to click the link under this first instruction and fill your Drive code to the notebook."]},{"cell_type":"code","metadata":{"id":"pSeoKUc4PCSF"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"r9ExCAgGS8ST"},"source":["!mkdir /content/drive/MyDrive/SSD-Tensorflow_logs"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iQqVR6b5gYVs"},"source":["Lastly we clean up the GPU cache. We are now ready to start the training."]},{"cell_type":"code","metadata":{"id":"7NZ5yk8mXROY"},"source":["!python -c \"import torch; torch.cuda.empty_cache()\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gKin1tm9gmrH"},"source":["#5. Training\n","These two blocks include the instructions to start the training.\n","\n","IMPORTANT: you have to run only the correct one, depending on the pre-treined chosen weights.\n","If \"out of memory\" error occurs, try to change \"batch_size\" with a lower value.\n","\n","If Google Colaboratory stops your run before the training has finished, you have to restart the training starting from the last weights. In order to do that, you have to replace line 9 with \"--checkpoint_path=../../content/drive/MyDrive/SSD-Tensorflow_logs/model.ckpt \\ \", for this you have to have your Google Drive disk still mounted."]},{"cell_type":"markdown","metadata":{"id":"ZyKFfW6577L3"},"source":["ssd-300"]},{"cell_type":"code","metadata":{"id":"XVwlMnIjiMmg"},"source":["%cd /home/SSD-Tensorflow/\n","\n","!python train_ssd_network.py \\\n","    --train_dir=../../content/drive/MyDrive/SSD-Tensorflow_logs \\\n","    --dataset_dir=./data/kitti_cones/tfrecord/ \\\n","    --dataset_name=kitti \\\n","    --dataset_split_name=train \\\n","    --model_name=ssd_300_vgg \\\n","    --checkpoint_path=./checkpoints/ssd_300_vgg.ckpt \\\n","    --save_summaries_secs=60 \\\n","    --save_interval_secs=600 \\\n","    --weight_decay=0.0005 \\\n","    --optimizer=adam \\\n","    --learning_rate=0.001 \\\n","    --batch_size=32"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"G4wAJ3FC78rP"},"source":["ssd-512"]},{"cell_type":"code","metadata":{"id":"tP7q9oUcZZ3o"},"source":["%cd /home/SSD-Tensorflow/\n","\n","!python train_ssd_network.py \\\n","    --train_dir=../../content/drive/MyDrive/SSD-Tensorflow_logs \\\n","    --dataset_dir=./data/kitti_cones/tfrecord/ \\\n","    --dataset_name=kitti \\\n","    --dataset_split_name=train \\\n","    --model_name=ssd_512_vgg \\\n","    --checkpoint_path=./checkpoints/VGG_VOC0712_SSD_512x512_ft_iter_120000.ckpt \\\n","    --save_summaries_secs=60 \\\n","    --save_interval_secs=600 \\\n","    --weight_decay=0.0005 \\\n","    --optimizer=adam \\\n","    --learning_rate=0.001 \\\n","    --batch_size=32"],"execution_count":null,"outputs":[]}]}